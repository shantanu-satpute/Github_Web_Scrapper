{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Project Introduction:\n",
        "\n",
        "\n",
        "1.   **Problem statement**: <br>\n",
        "The objective of this project is to extract and organize information from GitHub's topic pages. GitHub offers a wealth of information on various topics and associated repositories, but manually gathering this data can be time-consuming. The goal is to automate the process of collecting data, organizing it into structured formats, and saving it into CSV files for further analysis or usage.<br><br>\n",
        "\n",
        "\n",
        "2.   **Introduction to  Web Scraping and GitHub**: <br>\n",
        "GitHub is a popular platform for hosting and sharing code repositories, facilitating collaboration and version control. Web scraping involves extracting data from websites, allowing users to gather information in a structured format. In this project, web scraping will be used to extract data from GitHub's topic pages.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "3. **Tools used**:\n",
        "*   <u>Python</u>: Backbone of the whole project.\n",
        "\n",
        "*   <u>Requests</u>: Used to fetch the content of GitHub's topic pages.\n",
        "\n",
        "*   <u>BeautifulSoup</u>: It helps in navigating and searching through the HTML structure of GitHub pages.\n",
        "\n",
        "*   <u>Pandas</u>: Use to organize the extracted data into structured formats like CSV files.<br><br>\n",
        "\n",
        "\n",
        "4. **Useful Links**: <br>\n",
        "https://github.com <br>\n",
        "https://github.com/topics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X3lal5sJjpbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project outline:\n",
        "1.   We are going to scrap https://github.com/topics\n",
        "2.   We will get a list of topics. For each topic we will get topic title, topic page URL and topic description.\n",
        "\n",
        "3.   For each topic we will get top 20 repositories in the topic from the topic page.\n",
        "4. For each repositories we will grab the reponame, username(author name), stars and repo URL."
      ],
      "metadata": {
        "id": "RwmCNp0EnkOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scrape the list of topics from GitHub:"
      ],
      "metadata": {
        "id": "A65UWyFroU_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the requests library to download web pages:**"
      ],
      "metadata": {
        "id": "A0EP9XvqosUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests --upgrade --quiet"
      ],
      "metadata": {
        "id": "R_6UH_OUl8Sq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "74h9Jq_9ozSW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Beautiful Soup to parse and extract information**:"
      ],
      "metadata": {
        "id": "-05cPdmlrbXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 --upgrade --quiet"
      ],
      "metadata": {
        "id": "LoevsjB0rTOV"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "pLQq3eWdrVXF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create one function to download the webpage of gitHub topics**"
      ],
      "metadata": {
        "id": "6SpfYmsDrHpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_page():\n",
        "    topic_url = 'https://github.com/topics'\n",
        "    response = requests.get(topic_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Failed to load page {topic_url}')\n",
        "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return topic_doc"
      ],
      "metadata": {
        "id": "71decgajo1aw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parse the information from the topic page:"
      ],
      "metadata": {
        "id": "GKtkMcPwqBE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create some helper functions**:"
      ],
      "metadata": {
        "id": "3f-9M-psr6Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_title(doc):\n",
        "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tags = doc.find_all('p', class_=selection_class)\n",
        "\n",
        "    topic_titles = [tag.text.strip() for tag in topic_title_tags]\n",
        "\n",
        "    return topic_titles\n",
        "\n",
        "\n",
        "# topic_titles = get_topic_title(doc)\n",
        "# print(topic_titles)"
      ],
      "metadata": {
        "id": "6gFUzXV1qAdW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_topic_title can be used to get the list of titles"
      ],
      "metadata": {
        "id": "hATlWdLssMh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_url(doc):\n",
        "    topic_link_selector = 'no-underline flex-1 d-flex flex-column'\n",
        "    topic_link_tags = doc.find_all('a', class_=topic_link_selector)\n",
        "    base_url = 'https://github.com'\n",
        "    topic_urls = [base_url + tag['href'] for tag in topic_link_tags]\n",
        "    return topic_urls"
      ],
      "metadata": {
        "id": "rfX4PjjhqFUV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using get_topic_desc function to get the list of topic description"
      ],
      "metadata": {
        "id": "gx3u3nbmsM1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_desc(doc):\n",
        "    topic_desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
        "    topic_desc_tags = doc.find_all('p', class_=topic_desc_selector)\n",
        "    topic_descs = [tag.text.strip() for tag in topic_desc_tags]\n",
        "    return topic_descs"
      ],
      "metadata": {
        "id": "Qwo1SP6qqGje"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to topic title and topic url, we create the function get_topic_url to grab the url's of the topics."
      ],
      "metadata": {
        "id": "oYduw9HVsNRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing and importing pandas library to form the dataframes**:"
      ],
      "metadata": {
        "id": "F5qu2caftzpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --quiet"
      ],
      "metadata": {
        "id": "TOOswRnct33U"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "u5ATBVW0t6AN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine all these functions into a single one:"
      ],
      "metadata": {
        "id": "nrab8J8iteKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics():\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Failed to load page {topics_url}')\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    topics_dict = {\n",
        "        'Topic_title': get_topic_title(doc),\n",
        "        'Topic_description': get_topic_desc(doc),\n",
        "        'Topic_URL': get_topic_url(doc)\n",
        "    }\n",
        "    return pd.DataFrame(topics_dict)"
      ],
      "metadata": {
        "id": "nYtl3F7AtkeT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it to extract the information related to each topic."
      ],
      "metadata": {
        "id": "q8L7HmGjmhrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_df = scrape_topics()\n",
        "total_df.to_csv(f'topics.csv', index=None)"
      ],
      "metadata": {
        "id": "KFqb9ebWiViD"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get top 20 repositories in the topic from the topic page:"
      ],
      "metadata": {
        "id": "pdxQ5NVxuTdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create one function to download the webpage of each topic with the help of topic url**"
      ],
      "metadata": {
        "id": "k6_LspU6ukOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_page(topic_url):\n",
        "    response = requests.get(topic_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Failed to load page {topic_url}')\n",
        "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return topic_doc"
      ],
      "metadata": {
        "id": "OaNXMjJIuepu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful functions**:<br>\n",
        "\n",
        "* get_repo_info: It will be use to scrape the username, repo_name,\n",
        " repo_url and the count of stars on each topic.\n",
        "\n",
        "*  parse_star_count: This function will convert value of star count to a number. For example(3.2k to 3200).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjIdR4FJuoOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_star_count(stars_str):\n",
        "  stars_str = stars_str.strip()\n",
        "  if stars_str[-1] == 'k':\n",
        "    return int(float(stars_str[:-1])*1000)\n",
        "  else:\n",
        "    return int(stars_str)\n",
        "\n",
        "def get_repo_info(repo_tag, star_tag):\n",
        "    a_tags = repo_tag.find_all('a')\n",
        "    username = a_tags[0].text.strip()\n",
        "    repo_name = a_tags[1].text.strip()\n",
        "    repo_url = 'https://github.com' + a_tags[1]['href']\n",
        "    stars = parse_star_count(star_tag.text.strip())\n",
        "    return username, repo_name, stars, repo_url"
      ],
      "metadata": {
        "id": "q0Ejsj2PuwGV"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_repos(topic_doc):\n",
        "    repo_tags = topic_doc.find_all('h3', class_='f3 color-fg-muted text-normal lh-condensed')\n",
        "    star_tags = topic_doc.find_all('span', class_='Counter js-social-count')\n",
        "\n",
        "    topic_repos_dict = {\n",
        "        'username': [],\n",
        "        'repo_name': [],\n",
        "        'stars': [],\n",
        "        'repo_url': []\n",
        "    }\n",
        "\n",
        "    for i in range(len(repo_tags)):\n",
        "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
        "        topic_repos_dict['username'].append(repo_info[0])\n",
        "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
        "        topic_repos_dict['stars'].append(repo_info[2])\n",
        "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "    return pd.DataFrame(topic_repos_dict)"
      ],
      "metadata": {
        "id": "W_VlFdnFvJyj"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic(topic_url, topic_name):\n",
        "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "    topic_df.to_csv(f'{topic_name}.csv', index=None)"
      ],
      "metadata": {
        "id": "zizZ1StTvMMF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Puttting it all together**: <br>\n",
        "\n",
        "\n",
        "*  We have a function to get the list of topics.\n",
        "*  We have a function to create CSV file for scraped repos from topics page.\n",
        "\n",
        "*   Now let's put them all together.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MjowiNMSgsdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic_repos():\n",
        "    topics_df = scrape_topics()\n",
        "    print('Scraping list of topics:')\n",
        "    for index, row in topics_df.iterrows():\n",
        "        print(f'Scraping top repos for the topic: \"{row[\"Topic_title\"]}\"')\n",
        "        scrape_topic(row['Topic_URL'], row['Topic_title'])"
      ],
      "metadata": {
        "id": "jFzHT_EJvOAj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it to scrape the top repos for all the topics on the first page of https://github.com/topics"
      ],
      "metadata": {
        "id": "G9ubLxMyh8kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_topic_repos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRrYqPB1vPZ8",
        "outputId": "c1a93bbe-c4cd-4ed0-efe5-d7d682c48c1e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping list of topics:\n",
            "Scraping top repos for the topic: \"3D\"\n",
            "Scraping top repos for the topic: \"Ajax\"\n",
            "Scraping top repos for the topic: \"Algorithm\"\n",
            "Scraping top repos for the topic: \"Amp\"\n",
            "Scraping top repos for the topic: \"Android\"\n",
            "Scraping top repos for the topic: \"Angular\"\n",
            "Scraping top repos for the topic: \"Ansible\"\n",
            "Scraping top repos for the topic: \"API\"\n",
            "Scraping top repos for the topic: \"Arduino\"\n",
            "Scraping top repos for the topic: \"ASP.NET\"\n",
            "Scraping top repos for the topic: \"Awesome Lists\"\n",
            "Scraping top repos for the topic: \"Amazon Web Services\"\n",
            "Scraping top repos for the topic: \"Azure\"\n",
            "Scraping top repos for the topic: \"Babel\"\n",
            "Scraping top repos for the topic: \"Bash\"\n",
            "Scraping top repos for the topic: \"Bitcoin\"\n",
            "Scraping top repos for the topic: \"Bootstrap\"\n",
            "Scraping top repos for the topic: \"Bot\"\n",
            "Scraping top repos for the topic: \"C\"\n",
            "Scraping top repos for the topic: \"Chrome\"\n",
            "Scraping top repos for the topic: \"Chrome extension\"\n",
            "Scraping top repos for the topic: \"Command-line interface\"\n",
            "Scraping top repos for the topic: \"Clojure\"\n",
            "Scraping top repos for the topic: \"Code quality\"\n",
            "Scraping top repos for the topic: \"Code review\"\n",
            "Scraping top repos for the topic: \"Compiler\"\n",
            "Scraping top repos for the topic: \"Continuous integration\"\n",
            "Scraping top repos for the topic: \"C++\"\n",
            "Scraping top repos for the topic: \"Cryptocurrency\"\n",
            "Scraping top repos for the topic: \"Crystal\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References and Future Ideas:"
      ],
      "metadata": {
        "id": "zwOH32zVvj3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**: <br>\n",
        "*  BeautifulSoup Documentation: https://beautiful-soup-4.readthedocs.io/en/latest/\n",
        "\n",
        "* Request Documentation:  https://pypi.org/project/requests/\n",
        "\n",
        "*  Pandas Documentation: https://pandas.pydata.org/docs/user_guide/10min.html\n",
        "<br>\n",
        "\n",
        "**Future Ideas**:<br>\n",
        "*   Develop some functions in the same script to extract the data related to some more topics present on different pages of the same website.\n",
        "\n",
        "*   Make it more user friendly by asking the topic names to the user itself.\n",
        "\n",
        "*   Develop some mechanism for real-time data updates by setting up particular frequency.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2KwkmcGHiLBC"
      }
    }
  ]
}